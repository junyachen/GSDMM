{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "from numpy import log, exp\n",
    "from numpy import argmax\n",
    "import json\n",
    "\n",
    "class MovieGroupProcess:\n",
    "    def __init__(self, K=8, alpha=0.1, beta=0.1, n_iters=30):\n",
    "        '''\n",
    "        A MovieGroupProcess is a conceptual model introduced by Yin and Wang 2014 to\n",
    "        describe their Gibbs sampling algorithm for a Dirichlet Mixture Model for the\n",
    "        clustering short text documents.\n",
    "        Reference: http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/KDD14-GSDMM.pdf\n",
    "        Imagine a professor is leading a film class. At the start of the class, the students\n",
    "        are randomly assigned to K tables. Before class begins, the students make lists of\n",
    "        their favorite films. The teacher reads the role n_iters times. When\n",
    "        a student is called, the student must select a new table satisfying either:\n",
    "            1) The new table has more students than the current table.\n",
    "        OR\n",
    "            2) The new table has students with similar lists of favorite movies.\n",
    "        :param K: int\n",
    "            Upper bound on the number of possible clusters. Typically many fewer\n",
    "        :param alpha: float between 0 and 1\n",
    "            Alpha controls the probability that a student will join a table that is currently empty\n",
    "            When alpha is 0, no one will join an empty table.\n",
    "        :param beta: float between 0 and 1\n",
    "            Beta controls the student's affinity for other students with similar interests. A low beta means\n",
    "            that students desire to sit with students of similar interests. A high beta means they are less\n",
    "            concerned with affinity and are more influenced by the popularity of a table\n",
    "        :param n_iters:\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_iters = n_iters\n",
    "\n",
    "        # slots for computed variables\n",
    "        self.number_docs = None\n",
    "        self.vocab_size = None\n",
    "        self.cluster_doc_count = [0 for _ in range(K)]\n",
    "        self.cluster_word_count = [0 for _ in range(K)]\n",
    "        self.cluster_word_distribution = [{} for i in range(K)]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_data(K, alpha, beta, D, vocab_size, cluster_doc_count, cluster_word_count, cluster_word_distribution):\n",
    "        '''\n",
    "        Reconstitute a MovieGroupProcess from previously fit data\n",
    "        :param K:\n",
    "        :param alpha:\n",
    "        :param beta:\n",
    "        :param D:\n",
    "        :param vocab_size:\n",
    "        :param cluster_doc_count:\n",
    "        :param cluster_word_count:\n",
    "        :param cluster_word_distribution:\n",
    "        :return:\n",
    "        '''\n",
    "        mgp = MovieGroupProcess(K, alpha, beta, n_iters=30)\n",
    "        mgp.number_docs = D\n",
    "        mgp.vocab_size = vocab_size\n",
    "        mgp.cluster_doc_count = cluster_doc_count\n",
    "        mgp.cluster_word_count = cluster_word_count\n",
    "        mgp.cluster_word_distribution = cluster_word_distribution\n",
    "        return mgp\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample(p):\n",
    "        '''\n",
    "        Sample with probability vector p from a multinomial distribution\n",
    "        :param p: list\n",
    "            List of probabilities representing probability vector for the multinomial distribution\n",
    "        :return: int\n",
    "            index of randomly selected output\n",
    "        '''\n",
    "        return [i for i, entry in enumerate(multinomial(1, p)) if entry != 0][0]\n",
    "\n",
    "    def fit(self, docs, vocab_size):\n",
    "        '''\n",
    "        Cluster the input documents\n",
    "        :param docs: list of list\n",
    "            list of lists containing the unique token set of each document\n",
    "        :param V: total vocabulary size for each document\n",
    "        :return: list of length len(doc)\n",
    "            cluster label for each document\n",
    "        '''\n",
    "        alpha, beta, K, n_iters, V = self.alpha, self.beta, self.K, self.n_iters, vocab_size\n",
    "\n",
    "        D = len(docs)\n",
    "        self.number_docs = D\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # unpack to easy var names\n",
    "        m_z, n_z, n_z_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution\n",
    "        cluster_count = K\n",
    "        d_z = [None for i in range(len(docs))]\n",
    "\n",
    "        # initialize the clusters\n",
    "        for i, doc in enumerate(docs):\n",
    "\n",
    "            # choose a random  initial cluster for the doc\n",
    "            z = self._sample([1.0 / K for _ in range(K)])\n",
    "            d_z[i] = z\n",
    "            m_z[z] += 1\n",
    "            n_z[z] += len(doc)\n",
    "\n",
    "            for word in doc:\n",
    "                if word not in n_z_w[z]:\n",
    "                    n_z_w[z][word] = 0\n",
    "                n_z_w[z][word] += 1\n",
    "\n",
    "        for _iter in range(n_iters):\n",
    "            total_transfers = 0\n",
    "\n",
    "            for i, doc in enumerate(docs):\n",
    "\n",
    "                # remove the doc from it's current cluster\n",
    "                z_old = d_z[i]\n",
    "\n",
    "                m_z[z_old] -= 1\n",
    "                n_z[z_old] -= len(doc)\n",
    "\n",
    "                for word in doc:\n",
    "                    n_z_w[z_old][word] -= 1\n",
    "\n",
    "                    # compact dictionary to save space\n",
    "                    if n_z_w[z_old][word] == 0:\n",
    "                        del n_z_w[z_old][word]\n",
    "\n",
    "                # draw sample from distribution to find new cluster\n",
    "                p = self.score(doc)\n",
    "                z_new = self._sample(p)\n",
    "\n",
    "                # transfer doc to the new cluster\n",
    "                if z_new != z_old:\n",
    "                    total_transfers += 1\n",
    "\n",
    "                d_z[i] = z_new\n",
    "                m_z[z_new] += 1\n",
    "                n_z[z_new] += len(doc)\n",
    "\n",
    "                for word in doc:\n",
    "                    if word not in n_z_w[z_new]:\n",
    "                        n_z_w[z_new][word] = 0\n",
    "                    n_z_w[z_new][word] += 1\n",
    "\n",
    "            cluster_count_new = sum([1 for v in m_z if v > 0])\n",
    "            print(\"In stage %d: transferred %d clusters with %d clusters populated\" % (\n",
    "            _iter, total_transfers, cluster_count_new))\n",
    "            if total_transfers == 0 and cluster_count_new == cluster_count and _iter>25:\n",
    "                print(\"Converged.  Breaking out.\")\n",
    "                break\n",
    "            cluster_count = cluster_count_new\n",
    "        self.cluster_word_distribution = n_z_w\n",
    "        return d_z\n",
    "\n",
    "    def score(self, doc):\n",
    "        '''\n",
    "        Score a document\n",
    "        Implements formula (3) of Yin and Wang 2014.\n",
    "        http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/KDD14-GSDMM.pdf\n",
    "        :param doc: list[str]: The doc token stream\n",
    "        :return: list[float]: A length K probability vector where each component represents\n",
    "                              the probability of the document appearing in a particular cluster\n",
    "        '''\n",
    "        alpha, beta, K, V, D = self.alpha, self.beta, self.K, self.vocab_size, self.number_docs\n",
    "        m_z, n_z, n_z_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution\n",
    "\n",
    "        p = [0 for _ in range(K)]\n",
    "\n",
    "        #  We break the formula into the following pieces\n",
    "        #  p = N1*N2/(D1*D2) = exp(lN1 - lD1 + lN2 - lD2)\n",
    "        #  lN1 = log(m_z[z] + alpha)\n",
    "        #  lN2 = log(D - 1 + K*alpha)\n",
    "        #  lN2 = log(product(n_z_w[w] + beta)) = sum(log(n_z_w[w] + beta))\n",
    "        #  lD2 = log(product(n_z[d] + V*beta + i -1)) = sum(log(n_z[d] + V*beta + i -1))\n",
    "\n",
    "        lD1 = log(D - 1 + K * alpha)\n",
    "        doc_size = len(doc)\n",
    "        for label in range(K):\n",
    "            lN1 = log(m_z[label] + alpha)\n",
    "            lN2 = 0\n",
    "            lD2 = 0\n",
    "            for word in doc:\n",
    "                lN2 += log(n_z_w[label].get(word, 0) + beta)\n",
    "            for j in range(1, doc_size +1):\n",
    "                lD2 += log(n_z[label] + V * beta + j - 1)\n",
    "            p[label] = exp(lN1 - lD1 + lN2 - lD2)\n",
    "\n",
    "        # normalize the probability vector\n",
    "        pnorm = sum(p)\n",
    "        pnorm = pnorm if pnorm>0 else 1\n",
    "        return [pp/pnorm for pp in p]\n",
    "\n",
    "    def choose_best_label(self, doc):\n",
    "        '''\n",
    "        Choose the highest probability label for the input document\n",
    "        :param doc: list[str]: The doc token stream\n",
    "        :return:\n",
    "        '''\n",
    "        p = self.score(doc)\n",
    "        return argmax(p),max(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import numpy \n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, vectors, clf):\n",
    "        self.embeddings = vectors\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[int(x)] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y, Y_, average=average)\n",
    "        results['accuracy'] = accuracy_score(Y, Y_)\n",
    "        \n",
    "        # print('Results, using embeddings of dimensionality', len(self.embeddings[X[0]]))\n",
    "        # print('-------------------')\n",
    "        #print(results)\n",
    "        return results\n",
    "        # print('-------------------')\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = numpy.asarray([self.embeddings[int(x)] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = numpy.random.get_state()\n",
    "\n",
    "        training_size = int(train_precent * len(X))\n",
    "        numpy.random.seed(seed)\n",
    "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y)\n",
    "        numpy.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = numpy.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return numpy.asarray(all_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from time import time\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "filename = \"Tweet\"\n",
    "#filename = \"T\"\n",
    "#filename = \"S\"\n",
    "#filename = \"TS\"\n",
    "data_path = \"./../data/\"+filename\n",
    "data = []\n",
    "docs = []\n",
    "total_doc_num = 2472\n",
    "doc_num = 0\n",
    "doc_label = np.zeros((total_doc_num,),dtype=int)\n",
    "with open(data_path,'r')as file:\n",
    "    punc = '\",:'\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        line = re.sub(r'[{}]+'.format(punc),'',line)\n",
    "        \n",
    "        data.append(\" \".join(line.split(' ')[1:-2]))\n",
    "        doc_label[doc_num] = int(line.split(' ')[-1].strip().replace(\"}\",\"\"))\n",
    "        docs.append(line.split(' ')[1:-2])\n",
    "        \n",
    "        doc_num += 1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_set = set()\n",
    "for count in data:\n",
    "    for j in count.split(\" \"):\n",
    "        uni_set.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 2213 clusters with 89 clusters populated\n",
      "In stage 1: transferred 903 clusters with 86 clusters populated\n",
      "In stage 2: transferred 422 clusters with 81 clusters populated\n",
      "In stage 3: transferred 314 clusters with 76 clusters populated\n",
      "In stage 4: transferred 260 clusters with 74 clusters populated\n",
      "In stage 5: transferred 242 clusters with 73 clusters populated\n",
      "In stage 6: transferred 223 clusters with 69 clusters populated\n",
      "In stage 7: transferred 200 clusters with 68 clusters populated\n",
      "In stage 8: transferred 154 clusters with 67 clusters populated\n",
      "In stage 9: transferred 158 clusters with 67 clusters populated\n"
     ]
    }
   ],
   "source": [
    "n_cluster = 89\n",
    "beta = 0.1\n",
    "mgp = MovieGroupProcess(K=n_cluster, alpha=0.1, beta=beta, n_iters=10)\n",
    "y = mgp.fit(docs,len(uni_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgp_doc_label = np.zeros((total_doc_num,),dtype=int)\n",
    "for i in range(len(docs)):\n",
    "    mgp_doc_label[i] = mgp.choose_best_label(docs[i])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8545964687694341"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(np.array(doc_label), np.array(mgp_doc_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./../Tweet_dictionary.txt\"\n",
    "word_id = {}\n",
    "with open(filename,'r')as datafile:\n",
    "    sents = datafile.readlines()\n",
    "    for data in sents:\n",
    "        word_id[data.split(\" \")[0]] = int(data.split(\" \")[1].strip())\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.001\n",
    "topic_word_dis = np.zeros((n_cluster,len(uni_set)))\n",
    "for i in range(len(mgp.cluster_word_distribution)):\n",
    "    for j,keyword in enumerate(word_id):\n",
    "        if keyword not in mgp.cluster_word_distribution[i]:\n",
    "            topic_word_dis[i][j] = beta\n",
    "        else:\n",
    "            topic_word_dis[i][j] = mgp.cluster_word_distribution[i][keyword] + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_topic_word_dis = topic_word_dis / np.sum(topic_word_dis, axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "total_doc_num = 2472\n",
    "doc_emb = np.zeros((total_doc_num,n_cluster))\n",
    "doc_label = np.zeros((total_doc_num,),dtype=int)\n",
    "import re\n",
    "filename = \"./../data/Tweet\"\n",
    "doc_num = 0\n",
    "with open(filename,'r')as datafile:\n",
    "        sents = datafile.readlines()\n",
    "        punc = '\",:'\n",
    "        for data in sents:\n",
    "            \n",
    "            data = re.sub(r'[{}]+'.format(punc),'',data)\n",
    "            raw_text = data.split(' ')[1:-2]\n",
    "            \n",
    "            doc_label[doc_num] = int(data.split(' ')[-1].strip().replace(\"}\",\"\"))\n",
    "            for data_i in raw_text:\n",
    "                doc_emb[doc_num] += norm_topic_word_dis[:,word_id[data_i]]\n",
    "            doc_emb[doc_num] /= len(raw_text)  \n",
    "            #doc_emb[doc_num]\n",
    "            doc_num += 1\n",
    "datafile.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=89,  max_iter=100).fit(doc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8122011914659129\n",
      "0.55542071197411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print(normalized_mutual_info_score(np.array(doc_label), np.array(kmeans.labels_)))\n",
    "print(acc(np.array(doc_label), np.array(kmeans.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_nmi:  0.804737707149681\n",
      "avg_acc:  0.5222694174757281\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "avg_nmi = []\n",
    "avg_acc = []\n",
    "for i in range(20):\n",
    "    kmeans = KMeans(n_clusters=89,  max_iter=100).fit(doc_emb)\n",
    "    avg_nmi.append(normalized_mutual_info_score(np.array(doc_label), np.array(kmeans.labels_)))\n",
    "    avg_acc.append(acc(np.array(doc_label), np.array(kmeans.labels_)))\n",
    "print(\"avg_nmi: \", np.mean(avg_nmi))\n",
    "print(\"avg_acc: \",np.mean(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [str(i) for i in list(doc_label)]\n",
    "X = [str(i) for i in list(np.arange(total_doc_num))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.2994788650839606\n",
      "0.2   0.2984996738421396\n",
      "0.3   0.31233283803863304\n",
      "0.4   0.3583694709453599\n",
      "0.5   0.3044838373305526\n",
      "0.6   0.31134289439374185\n",
      "0.7   0.3647140864714086\n",
      "0.8   0.36649214659685864\n",
      "0.9   0.4033970276008492\n",
      "0.33545676003372266\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['micro'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['micro'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.13313127347008674\n",
      "0.2   0.13315736843600212\n",
      "0.3   0.1444896236650231\n",
      "0.4   0.1625124967329073\n",
      "0.5   0.13919554919649346\n",
      "0.6   0.1469360587268253\n",
      "0.7   0.18359720266543103\n",
      "0.8   0.1958220438211641\n",
      "0.9   0.20096162887981545\n",
      "0.15997813839930541\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['macro'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['macro'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1   0.02696629213483146\n",
      "0.2   0.03083923154701719\n",
      "0.3   0.04737146158290006\n",
      "0.4   0.1078167115902965\n",
      "0.5   0.07605177993527508\n",
      "0.6   0.07987866531850354\n",
      "0.7   0.11185983827493262\n",
      "0.8   0.09090909090909091\n",
      "0.9   0.12903225806451613\n",
      "0.07785836992859595\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "avg_value = []\n",
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "#for i in np.arange(0.01, 0.11, 0.01):\n",
    "    clf_ratio = np.round(i,2) \n",
    "    clf = Classifier(vectors=doc_emb, clf=LogisticRegression())\n",
    "    avg_value.append(clf.split_train_evaluate(X, Y, clf_ratio)['accuracy'])\n",
    "    print( clf_ratio,\" \",clf.split_train_evaluate(X, Y, clf_ratio)['accuracy'] )\n",
    "print(np.mean(avg_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "storefile = 'emb/Tweet_gsdmm_emb.txt'\n",
    "sf = open(storefile,'w')\n",
    "for i in range(doc_emb.shape[0]):\n",
    "    sf.write(str(i)+\" \"+\" \".join([str(ele) for ele in doc_emb[i]])+'\\n')\n",
    "sf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
